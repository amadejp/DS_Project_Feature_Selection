{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rank_eval_pipeline import RankEval\n",
    "import rank_algos\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "from helper_functions import get_true_baseline, area_under_the_curve\n",
    "\n",
    "import time\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_ranking(rankings):\n",
    "    \"\"\"\n",
    "    Using the borda count method, average the rankings in the list of rankings.\n",
    "\n",
    "    Returns: a list of features in order of their average ranking,\n",
    "                a list of the average scores for each feature\n",
    "    \"\"\"\n",
    "    scores = defaultdict(int)\n",
    "    for ranking in rankings:\n",
    "        for i, feature in enumerate(ranking):\n",
    "            scores[feature] += len(ranking) - i\n",
    "\n",
    "    # sort by score, highest first\n",
    "    average_ranking = sorted(scores.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "    # extract the features and their scores from the (feature, score) pairs\n",
    "    average_features = [feature for feature, score in average_ranking]\n",
    "    average_scores = [score for feature, score in average_ranking]\n",
    "\n",
    "    return average_features, average_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ranking(features, scores):\n",
    "    \"\"\"\n",
    "    Takes feature ranking and scores and returns the AUC of the first generation and singles as well as the execution time if\n",
    "    it was passed to the function.\n",
    "    \"\"\"\n",
    "    RE = RankEval(\"\",\"\")\n",
    "    RE.ranking = features\n",
    "    RE.scores = scores\n",
    "    RE.evaluate_ranking()\n",
    "\n",
    "    baseline = get_true_baseline()\n",
    "\n",
    "    auc_first_gen = np.mean(RE.eval_res_first_gen[0] - baseline)/(1 - area_under_the_curve(baseline))\n",
    "\n",
    "    return auc_first_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_regular(subsampling):\n",
    "    start_time = time.time()\n",
    "    # load data\n",
    "    data = pd.read_csv('data/full_data.csv')\n",
    "\n",
    "    # create the RankEval object\n",
    "    RE = RankEval(data, rank_algos.random_forest_score,\n",
    "                    subsampling_proportion=subsampling)\n",
    "\n",
    "    # get the scores\n",
    "    features, scores = RE.get_scores()[0], RE.get_scores()[1]\n",
    "\n",
    "    end_time = time.time()\n",
    "    return evaluate_ranking(features, scores), end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_ensemble(seeds, subsampling):\n",
    "    start_time = time.time()\n",
    "    # load data\n",
    "    data = pd.read_csv('data/full_data.csv')\n",
    "\n",
    "    all_rankings = []\n",
    "\n",
    "    for seed in seeds:\n",
    "        # create the RankEval object\n",
    "        RE = RankEval(data, rank_algos.random_forest_score,\n",
    "                      seed=seed, \n",
    "                      subsampling_proportion=subsampling)\n",
    "\n",
    "        # get the scores\n",
    "        results = RE.get_scores()\n",
    "        all_rankings.append(list(results[0]))\n",
    "\n",
    "    avg = average_ranking(all_rankings)\n",
    "\n",
    "    end_time = time.time()\n",
    "    return evaluate_ranking(*avg), end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = {}\n",
    "# subsampling = []\n",
    "# seeds = list(range(100))\n",
    "# for sub in subsampling:\n",
    "#     print(f\"Subsampling: {sub}\")\n",
    "#     print(f\"Number of runs: 100 , approximated by {len(seeds)}\")\n",
    "#     time = timeit.timeit('main_ensemble(seeds, sub)', number=1, globals=globals())\n",
    "#     time_without_dataread = time -  12.6\n",
    "#     time_per_seed = time_without_dataread/len(seeds)\n",
    "#     time_for_100 = 12.6 + time_per_seed*100\n",
    "#     print(f\"Ensemble runtime: {time_for_100} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsampling: 0.0001\n",
      "Number of runs: 10\n",
      "Ensemble runtime: 16.044456958770752 seconds.\n",
      "Ensemble AUC: 0.4028707745528648\n",
      "Subsampling: 0.001\n",
      "Number of runs: 10\n",
      "Ensemble runtime: 15.81665825843811 seconds.\n",
      "Ensemble AUC: 0.4838189506482014\n",
      "Subsampling: 0.01\n",
      "Number of runs: 10\n",
      "Ensemble runtime: 28.3675217628479 seconds.\n",
      "Ensemble AUC: 0.5029717173636098\n",
      "Subsampling: 0.1\n",
      "Number of runs: 10\n",
      "Ensemble runtime: 189.19193720817566 seconds.\n",
      "Ensemble AUC: 0.5051891350445996\n"
     ]
    }
   ],
   "source": [
    "# results = {}\n",
    "# subsampling = [0.0001, 0.001, 0.01, 0.1]\n",
    "# seeds = list(range(10))\n",
    "# for sub in subsampling:\n",
    "#     print(f\"Subsampling: {sub}\")\n",
    "#     print((f\"Number of runs: {len(seeds)}\"))\n",
    "#     auc, runtime = main_ensemble(seeds, sub)\n",
    "#     print(f\"Ensemble runtime: {runtime} seconds.\")\n",
    "#     print(f\"Ensemble AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
